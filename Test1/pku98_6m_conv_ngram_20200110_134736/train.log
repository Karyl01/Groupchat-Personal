20-01-02 19:27:06 INFO Hyperparameter:
{
  "use_char": false,
  "run_eagerly": true,
  "epochs": 1,
  "batch_size": 100,
  "metrics": "f1",
  "optimizer": {
    "class_name": "Adam",
    "config": {
      "name": "Adam",
      "clipnorm": 5,
      "learning_rate": 0.001,
      "decay": 0.0,
      "beta_1": 0.9,
      "beta_2": 0.999,
      "epsilon": 1e-08,
      "amsgrad": false
    }
  },
  "loss": null,
  "weight_norm": true,
  "dropout_hidden": 0.2,
  "dropout_embed": 0.2,
  "filters": [
    200,
    200,
    200,
    200,
    200
  ],
  "kernel_size": 3,
  "window_size": 0,
  "embedding_trainable": true,
  "ngram_embed": 50,
  "word_embed": {
    "class_name": "HanLP>Word2VecEmbedding",
    "config": {
      "trainable": false,
      "filepath": "https://file.hankcs.com/hanlp/embeddings/radical_char_vec_20191229_013849.zip#character.vec.txt",
      "expand_vocab": true,
      "lowercase": false
    }
  }
}
20-01-02 19:27:23 INFO Vocab summary:
word_vocab[5753] = ['<pad>', '<unk>', '迈', '向', '充', '满', '希', '望', '的', '新', '世', '纪', '—', '一', '九', '八', '年', '讲', '话', '（', '附', '图', '片', '１', '张', '）', '中', '共', '央', '总', '书', '记', '、', '国', '家', '主', '席', '江', '泽', '民', '七', '十', '二', '月', '三', '日', '２', '３', '，', '发']
tag_vocab[4] = ['B', 'E', 'S', 'M']
20-01-02 19:27:23 INFO Building...
20-01-02 19:27:25 INFO Model built:
Model: "ngram_conv_tagging_model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
character.vec (Word2VecEmbed multiple                  1179900   
_________________________________________________________________
dropout (Dropout)            multiple                  0         
_________________________________________________________________
Conv1Dw_0_norm (WeightNormal multiple                  60401     
_________________________________________________________________
Conv1Dw_1_norm (WeightNormal multiple                  120401    
_________________________________________________________________
Conv1Dw_2_norm (WeightNormal multiple                  120401    
_________________________________________________________________
Conv1Dw_3_norm (WeightNormal multiple                  120401    
_________________________________________________________________
Conv1Dw_4_norm (WeightNormal multiple                  120401    
_________________________________________________________________
Conv1Dv_0_norm (WeightNormal multiple                  60401     
_________________________________________________________________
Conv1Dv_1_norm (WeightNormal multiple                  120401    
_________________________________________________________________
Conv1Dv_2_norm (WeightNormal multiple                  120401    
_________________________________________________________________
Conv1Dv_3_norm (WeightNormal multiple                  120401    
_________________________________________________________________
Conv1Dv_4_norm (WeightNormal multiple                  120401    
_________________________________________________________________
dropout_1 (Dropout)          multiple                  0         
_________________________________________________________________
dense (Dense)                multiple                  800       
=================================================================
Total params: 2,264,710
Trainable params: 1,084,800
Non-trainable params: 1,179,910
_________________________________________________________________
epoch |     f1 |   loss | val_f1 | val_loss
----- | ------ | ------ | ------ | --------
    1 | 0.8232 | 6.6660 | 0.9222 |  19.4778
20-01-02 19:40:12 INFO Trained 1 epochs in 12 m 47 s, each epoch takes 12 m 47 s
20-01-02 21:07:56 INFO Hyperparameter:
{
  "use_char": false,
  "run_eagerly": true,
  "epochs": 100,
  "batch_size": 100,
  "metrics": "f1",
  "optimizer": {
    "class_name": "Adam",
    "config": {
      "name": "Adam",
      "clipnorm": 5,
      "learning_rate": 0.001,
      "decay": 0.0,
      "beta_1": 0.9,
      "beta_2": 0.999,
      "epsilon": 1e-08,
      "amsgrad": false
    }
  },
  "loss": null,
  "weight_norm": true,
  "dropout_hidden": 0.2,
  "dropout_embed": 0.2,
  "filters": [
    200,
    200,
    200,
    200,
    200
  ],
  "kernel_size": 3,
  "window_size": 0,
  "embedding_trainable": true,
  "ngram_embed": 50,
  "word_embed": {
    "class_name": "HanLP>Word2VecEmbedding",
    "config": {
      "trainable": false,
      "filepath": "https://file.hankcs.com/hanlp/embeddings/radical_char_vec_20191229_013849.zip#character.vec.txt",
      "expand_vocab": true,
      "lowercase": false
    }
  }
}
20-01-02 21:08:13 INFO Vocab summary:
word_vocab[5753] = ['<pad>', '<unk>', '迈', '向', '充', '满', '希', '望', '的', '新', '世', '纪', '—', '一', '九', '八', '年', '讲', '话', '（', '附', '图', '片', '１', '张', '）', '中', '共', '央', '总', '书', '记', '、', '国', '家', '主', '席', '江', '泽', '民', '七', '十', '二', '月', '三', '日', '２', '３', '，', '发']
tag_vocab[4] = ['B', 'E', 'S', 'M']
20-01-02 21:08:13 INFO Building...
20-01-02 21:08:14 INFO Model built:
Model: "ngram_conv_tagging_model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
character.vec (Word2VecEmbed multiple                  1179900   
_________________________________________________________________
dropout (Dropout)            multiple                  0         
_________________________________________________________________
Conv1Dw_0_norm (WeightNormal multiple                  60401     
_________________________________________________________________
Conv1Dw_1_norm (WeightNormal multiple                  120401    
_________________________________________________________________
Conv1Dw_2_norm (WeightNormal multiple                  120401    
_________________________________________________________________
Conv1Dw_3_norm (WeightNormal multiple                  120401    
_________________________________________________________________
Conv1Dw_4_norm (WeightNormal multiple                  120401    
_________________________________________________________________
Conv1Dv_0_norm (WeightNormal multiple                  60401     
_________________________________________________________________
Conv1Dv_1_norm (WeightNormal multiple                  120401    
_________________________________________________________________
Conv1Dv_2_norm (WeightNormal multiple                  120401    
_________________________________________________________________
Conv1Dv_3_norm (WeightNormal multiple                  120401    
_________________________________________________________________
Conv1Dv_4_norm (WeightNormal multiple                  120401    
_________________________________________________________________
dropout_1 (Dropout)          multiple                  0         
_________________________________________________________________
dense (Dense)                multiple                  800       
=================================================================
Total params: 2,264,710
Trainable params: 1,084,800
Non-trainable params: 1,179,910
_________________________________________________________________
epoch |     f1 |   loss | val_f1 | val_loss
----- | ------ | ------ | ------ | --------
    1 | 0.8218 | 6.7168 | 0.9177 |  20.0740
    2 | 0.9045 | 3.7347 | 0.9398 |  15.6922
    3 | 0.9194 | 3.1645 | 0.9518 |  13.3528
    4 | 0.9278 | 2.8378 | 0.9591 |  12.7334
    5 | 0.9333 | 2.6213 | 0.9636 |  10.9560
    6 | 0.9373 | 2.4619 | 0.9662 |  10.7383
    7 | 0.9403 | 2.3451 | 0.9672 |   9.7387
    8 | 0.9424 | 2.2641 | 0.9688 |   9.1546
    9 | 0.9441 | 2.2005 | 0.9698 |   8.2269
   10 | 0.9454 | 2.1452 | 0.9709 |   8.4968
   11 | 0.9466 | 2.1008 | 0.9717 |   7.7164
   12 | 0.9474 | 2.0652 | 0.9727 |   7.8156
   13 | 0.9481 | 2.0366 | 0.9729 |   6.9606
   14 | 0.9488 | 2.0092 | 0.9720 |   6.7963
   15 | 0.9495 | 1.9836 | 0.9727 |   6.6611
   16 | 0.9501 | 1.9634 | 0.9734 |   6.4588
   17 | 0.9505 | 1.9427 | 0.9728 |   6.3013
   18 | 0.9509 | 1.9250 | 0.9732 |   6.1318
   19 | 0.9515 | 1.9065 | 0.9765 |   5.5060
   20 | 0.9518 | 1.8939 | 0.9753 |   5.4820
   21 | 0.9521 | 1.8788 | 0.9764 |   5.2919
   22 | 0.9525 | 1.8624 | 0.9759 |   5.4539
   23 | 0.9528 | 1.8553 | 0.9757 |   5.4062
   24 | 0.9531 | 1.8448 | 0.9768 |   5.2741
   25 | 0.9533 | 1.8327 | 0.9760 |   5.1861
   26 | 0.9534 | 1.8265 | 0.9756 |   4.7782
   27 | 0.9539 | 1.8124 | 0.9775 |   4.9700
   28 | 0.9540 | 1.8037 | 0.9768 |   4.9926
   29 | 0.9542 | 1.7983 | 0.9774 |   4.7050
   30 | 0.9545 | 1.7840 | 0.9770 |   4.6451
   31 | 0.9547 | 1.7765 | 0.9770 |   4.7778
   32 | 0.9548 | 1.7717 | 0.9802 |   4.4801
   33 | 0.9551 | 1.7618 | 0.9767 |   4.6312
   34 | 0.9553 | 1.7554 | 0.9766 |   4.5972
   35 | 0.9555 | 1.7515 | 0.9763 |   4.6294
   36 | 0.9556 | 1.7431 | 0.9760 |   4.5959
   37 | 0.9558 | 1.7378 | 0.9819 |   4.3050
   38 | 0.9558 | 1.7320 | 0.9781 |   4.2735
   39 | 0.9559 | 1.7277 | 0.9777 |   4.3233
   40 | 0.9561 | 1.7213 | 0.9785 |   4.3478
   41 | 0.9563 | 1.7171 | 0.9793 |   4.2228
   42 | 0.9563 | 1.7126 | 0.9794 |   4.1373
   43 | 0.9565 | 1.7066 | 0.9795 |   4.1533
   44 | 0.9566 | 1.7005 | 0.9807 |   4.0683
   45 | 0.9568 | 1.6949 | 0.9800 |   4.0336
   46 | 0.9569 | 1.6907 | 0.9837 |   3.9185
   47 | 0.9569 | 1.6888 | 0.9826 |   4.0195
   48 | 0.9571 | 1.6804 | 0.9820 |   3.8557
   49 | 0.9572 | 1.6789 | 0.9820 |   3.8710
   50 | 0.9574 | 1.6698 | 0.9815 |   3.8706
   51 | 0.9575 | 1.6677 | 0.9824 |   3.9326
   52 | 0.9575 | 1.6629 | 0.9837 |   3.8430
   53 | 0.9577 | 1.6584 | 0.9841 |   3.6532
   54 | 0.9577 | 1.6583 | 0.9843 |   3.8146
   55 | 0.9578 | 1.6541 | 0.9857 |   3.6866
   56 | 0.9579 | 1.6502 | 0.9851 |   3.5663
   57 | 0.9581 | 1.6418 | 0.9841 |   3.6324
   58 | 0.9581 | 1.6411 | 0.9861 |   3.4691
   59 | 0.9583 | 1.6397 | 0.9858 |   3.5522
   60 | 0.9582 | 1.6407 | 0.9857 |   3.5214
   61 | 0.9584 | 1.6335 | 0.9861 |   3.5143
   62 | 0.9583 | 1.6319 | 0.9863 |   3.4998
   63 | 0.9585 | 1.6255 | 0.9861 |   3.4275
   64 | 0.9585 | 1.6278 | 0.9858 |   3.4637
   65 | 0.9586 | 1.6229 | 0.9860 |   3.4436
   66 | 0.9587 | 1.6186 | 0.9869 |   3.2908
   67 | 0.9588 | 1.6158 | 0.9865 |   3.3838
   68 | 0.9589 | 1.6131 | 0.9865 |   3.3516
   69 | 0.9590 | 1.6072 | 0.9866 |   3.4246
   70 | 0.9590 | 1.6110 | 0.9867 |   3.4276
   71 | 0.9591 | 1.6000 | 0.9873 |   3.3888
   72 | 0.9590 | 1.6061 | 0.9869 |   3.3058
   73 | 0.9592 | 1.6018 | 0.9871 |   3.3098
   74 | 0.9592 | 1.5994 | 0.9869 |   3.2606
   75 | 0.9592 | 1.5986 | 0.9876 |   3.2604
   76 | 0.9592 | 1.5953 | 0.9872 |   3.3227
   77 | 0.9595 | 1.5888 | 0.9881 |   3.2972
   78 | 0.9595 | 1.5878 | 0.9882 |   3.2642
   79 | 0.9595 | 1.5879 | 0.9877 |   3.3574
   80 | 0.9597 | 1.5825 | 0.9878 |   3.2687
   81 | 0.9595 | 1.5835 | 0.9874 |   3.2034
   82 | 0.9596 | 1.5824 | 0.9882 |   3.0922
   83 | 0.9597 | 1.5763 | 0.9885 |   3.1986
   84 | 0.9597 | 1.5786 | 0.9880 |   3.2386
   85 | 0.9598 | 1.5723 | 0.9883 |   3.1668
   86 | 0.9599 | 1.5724 | 0.9882 |   3.1678
   87 | 0.9600 | 1.5698 | 0.9878 |   3.1250
   88 | 0.9600 | 1.5667 | 0.9879 |   3.0347
   89 | 0.9602 | 1.5656 | 0.9877 |   3.0188
   90 | 0.9601 | 1.5613 | 0.9882 |   2.9786
   91 | 0.9602 | 1.5632 | 0.9885 |   2.9721
   92 | 0.9604 | 1.5556 | 0.9883 |   3.0237
   93 | 0.9602 | 1.5566 | 0.9881 |   3.0047
   94 | 0.9603 | 1.5540 | 0.9890 |   2.9673
   95 | 0.9604 | 1.5514 | 0.9883 |   3.0555
   96 | 0.9603 | 1.5534 | 0.9877 |   3.0665
   97 | 0.9605 | 1.5482 | 0.9874 |   2.9893
   98 | 0.9606 | 1.5461 | 0.9886 |   3.0181
   99 | 0.9606 | 1.5426 | 0.9881 |   3.0081
  100 | 0.9606 | 1.5400 | 0.9885 |   2.9655
20-01-03 14:54:27 INFO Trained 100 epochs in 17 h 46 m 12 s, each epoch takes 10 m 40 s
20-01-03 14:54:27 INFO Restored the best model saved with best val_f1 = 0.9890 saved 7 epochs ago
20-01-09 20:42:20 INFO Hyperparameter:
{
  "use_char": false,
  "run_eagerly": true,
  "epochs": 100,
  "batch_size": 100,
  "metrics": "f1",
  "optimizer": {
    "class_name": "Adam",
    "config": {
      "name": "Adam",
      "clipnorm": 5,
      "learning_rate": 0.001,
      "decay": 0.0,
      "beta_1": 0.9,
      "beta_2": 0.999,
      "epsilon": 1e-08,
      "amsgrad": false
    }
  },
  "loss": null,
  "weight_norm": true,
  "dropout_hidden": 0.2,
  "dropout_embed": 0.2,
  "filters": [
    200,
    200,
    200,
    200,
    200
  ],
  "kernel_size": 3,
  "window_size": 0,
  "embedding_trainable": true,
  "ngram_embed": 50,
  "word_embed": {
    "class_name": "HanLP>Word2VecEmbedding",
    "config": {
      "trainable": false,
      "filepath": "https://file.hankcs.com/hanlp/embeddings/radical_char_vec_20191229_013849.zip#character.vec.txt",
      "expand_vocab": true,
      "lowercase": false
    }
  },
  "early_stopping_patience": 5
}
20-01-09 20:42:37 INFO Vocab summary:
word_vocab[5617] = ['<pad>', '<unk>', '迈', '向', '充', '满', '希', '望', '的', '新', '世', '纪', '—', '一', '九', '八', '年', '讲', '话', '(', '附', '图', '片', '1', '张', ')', '中', '共', '央', '总', '书', '记', ',', '国', '家', '主', '席', '江', '泽', '民', '七', '十', '二', '月', '三', '日', '2', '3', '发', '表']
tag_vocab[4] = ['B', 'E', 'S', 'M']
20-01-09 20:42:37 INFO Building...
20-01-09 20:42:39 INFO Model built:
Model: "ngram_conv_tagging_model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
character.vec (Word2VecEmbed multiple                  1166500   
_________________________________________________________________
dropout (Dropout)            multiple                  0         
_________________________________________________________________
Conv1Dw_0_norm (WeightNormal multiple                  60401     
_________________________________________________________________
Conv1Dw_1_norm (WeightNormal multiple                  120401    
_________________________________________________________________
Conv1Dw_2_norm (WeightNormal multiple                  120401    
_________________________________________________________________
Conv1Dw_3_norm (WeightNormal multiple                  120401    
_________________________________________________________________
Conv1Dw_4_norm (WeightNormal multiple                  120401    
_________________________________________________________________
Conv1Dv_0_norm (WeightNormal multiple                  60401     
_________________________________________________________________
Conv1Dv_1_norm (WeightNormal multiple                  120401    
_________________________________________________________________
Conv1Dv_2_norm (WeightNormal multiple                  120401    
_________________________________________________________________
Conv1Dv_3_norm (WeightNormal multiple                  120401    
_________________________________________________________________
Conv1Dv_4_norm (WeightNormal multiple                  120401    
_________________________________________________________________
dropout_1 (Dropout)          multiple                  0         
_________________________________________________________________
dense (Dense)                multiple                  800       
=================================================================
Total params: 2,251,310
Trainable params: 1,084,800
Non-trainable params: 1,166,510
_________________________________________________________________
epoch |     f1 |   loss | val_f1 | val_loss
----- | ------ | ------ | ------ | --------
    1 | 0.8251 | 6.6364 | 0.9301 |  17.9334
    2 | 0.9060 | 3.6764 | 0.9437 |  14.6103
    3 | 0.9210 | 3.0934 | 0.9555 |  12.4138
    4 | 0.9294 | 2.7669 | 0.9613 |  11.0725
    5 | 0.9349 | 2.5509 | 0.9649 |   9.9225
    6 | 0.9390 | 2.3959 | 0.9682 |   9.3474
    7 | 0.9416 | 2.2890 | 0.9692 |   9.3579
    8 | 0.9438 | 2.2034 | 0.9706 |   8.2878
    9 | 0.9453 | 2.1465 | 0.9732 |   7.5428
   10 | 0.9467 | 2.0866 | 0.9734 |   7.3710
   11 | 0.9477 | 2.0490 | 0.9728 |   6.7660
   12 | 0.9485 | 2.0179 | 0.9727 |   6.2668
   13 | 0.9493 | 1.9843 | 0.9749 |   6.2923
   14 | 0.9499 | 1.9609 | 0.9766 |   5.8158
   15 | 0.9505 | 1.9399 | 0.9740 |   5.9746
   16 | 0.9511 | 1.9162 | 0.9748 |   5.9517
   17 | 0.9515 | 1.8994 | 0.9764 |   5.3822
   18 | 0.9522 | 1.8779 | 0.9741 |   5.5026
   19 | 0.9523 | 1.8679 | 0.9746 |   5.3630
20-01-10 00:01:39 INFO Trained 19 epochs in 3 h 18 m 59 s, each epoch takes 10 m 28 s
20-01-10 00:01:39 INFO Restored the best model saved with best val_f1 = 0.9766 saved 6 epochs ago
